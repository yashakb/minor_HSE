{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как связаны мат. статистика и анализ данных?\n",
    "\n",
    "Пусть\n",
    "* $X$ — множество объектов, \n",
    "* $Y$ — конечное множество имён классов.\n",
    "\n",
    "Предположим, что $X \\times Y$ является вероятностным пространством с заданной на нём плотностью распределения \n",
    "\n",
    "$p(x, y) = P(y)p(x|y)$.\n",
    "\n",
    "Вероятности появления объектов каждого из классов\n",
    "\n",
    "$P_y = P(y)$\n",
    "\n",
    "называются **априорными вероятностями классов**. Плотности распределения \n",
    "\n",
    "$p_y(x) = p(x|y)$\n",
    "\n",
    "называются **функциями правдоподобия классов**.\n",
    "\n",
    "\n",
    "Пусть известны априорные вероятности классов $P_y$ и их функции правдоподобия $p_y(x)$, тогда для нахождения ответа на новом объекте $x$ будем максимизировать $p(y|x)$:\n",
    "\n",
    "$a(x) = \\arg max_{y} \\frac{p(x|y) P(y)}{p(x)} = \\arg max_{y} p(x|y) P(y).$\n",
    "\n",
    "Проблема в том, что мы, как правило, не знаем $P_y$ и $p_y(x)$, поэтому их приходится _оценивать_.\n",
    "\n",
    "* $P_y$ для каждого класса $y \\in Y$ можно оценить следующим образом:\n",
    "\n",
    "$\\hat{P}_y = \\frac{l_y}{l},$\n",
    "\n",
    "где\n",
    "\n",
    "$l_y$ — количество объектов класса $y$ в обучающей выборке,\n",
    "\n",
    "$l$ — общее количество объектов в обучающей выборке.\n",
    "\n",
    "По закону больших чисел (ЗБЧ) данная оценка будет стремится к априорной вероятности класса при увеличении обучающей выборки.\n",
    "\n",
    "* Существуют **параметрический** и **непараметрический** подходы к оцениванию функции правдоподобия класса.\n",
    "\n",
    "1) Параметрический подход предполагает, что распределение объектов данного класса взято из некоторого заданного семейства (например, нормальное распределение). Таким образом, необходимо лишь оценить его параметры (например, при помощи метода максимального правдоподобия).\n",
    "\n",
    "2) Непараметрический подход — нечто близкое к построению гистограммы, однако чуть более усложненной. Рассмотрим несколько вариантов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Непараметрические оценки плотности\n",
    "\n",
    "Будем рассматривать одномерную выборку $(x_1, ... x_m), x_m \\in \\mathbb{R}$ из распределения с плотностью $p(x)$.\n",
    "\n",
    "1) Дискретный случай\n",
    "\n",
    "Пусть $X$ — конечное множество. Тогда оценкой плотности может служить гистограмма значений:\n",
    "\n",
    "$$\\hat{p}(x) = \\frac{1}{l} \\sum_{i = 1}^m [x_i = x]$$\n",
    "\n",
    "2) Непрерывный случай\n",
    "\n",
    "Пусть $X = \\mathbb{R}.$ Тогда предыдущую оценку можно немного модифицировать:\n",
    "\n",
    "$$\\hat{p}(x) = \\frac{1}{l \\cdot h} \\sum_{i = 1}^m [|x - x_i| < h],$$\n",
    "\n",
    "где $h > 0$ — параметр, _ширина окна_.\n",
    "\n",
    "3) Локальная непараметрическая оценка Парзена-Розенблатта\n",
    "\n",
    "Оценка из предыдущего пункта является кусочно-постоянной. Чтобы избежать этого, используют следующую оценку:\n",
    "\n",
    "$$\\hat{p}(x) = \\frac{1}{l \\cdot h} \\sum_{i = 1}^m K(\\frac{x - x_i}{h}),$$\n",
    "\n",
    "где $K(z)$ — функция, называемая ядром.\n",
    "\n",
    "Таким образом, для восстановления плотности распределения в точке $x$ используются лишь те точки обучающей выборки, которые попали в окно заданной ширины $h$ вокруг $x$, причём чем ближе точка к $x$ — тем выше её вклад.\n",
    "\n",
    "<img src = \"kernels.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наивный байесовский классификатор\n",
    "\n",
    "Итого, у нас имеются различные способы оценки одномерных выборок. Что делать, если выборка имеет более 1 признака?\n",
    "\n",
    "Будем использовать **наивный байесовский классификатор**. Его идея заключается в том, что все признаки являются независимыми, следовательно, можно записать следующее выражение для функции правдоподобия произвольного класса:\n",
    "\n",
    "$$p_y(x) = \\prod_{i=1}^d p_{yi} (x^i) = p_{y1}(x^1) \\cdot ... \\cdot p_{yd}(x^d),$$\n",
    "\n",
    "где\n",
    "\n",
    "$x = (x^1, ... x^d) \\in X,$\n",
    "\n",
    "$p_{yi}(s)$ — плотность распределения $i$-ого признака среди объектов класса $y$.\n",
    "\n",
    "То есть оценку функции правдоподобия класса $y$ можно записать следующим образом:\n",
    "\n",
    "$$\\hat{p}_y(x) = \\prod_{i=1}^d \\hat{p}_{yi} (x^i) = \\hat{p}_{y1}(x^1) \\cdot ... \\cdot \\hat{p}_{yd}(x^d),$$\n",
    "\n",
    "где $\\hat{p}_{yi}(s)$ — оценка плотности распределения $i$-ого признака объектов класса $y$, построенная согласно приведенным ранее способам.\n",
    "\n",
    "Таким образом для построения алгоритма необходимо:\n",
    "\n",
    "1) для каждого класса $y \\in Y$ оценить $P_y$;\n",
    "\n",
    "2) для каждого класса $y \\in Y$ и каждого признака $i \\in {1, ..., d}$ оценить $p_{yi}(s)$;\n",
    "\n",
    "3) вычислить $a(x)$ по указанной выше формуле на новых объектах.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
